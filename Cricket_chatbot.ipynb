{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- 1. Data Setup (Loading intents.json from file) ---\n",
        "INTENTS_FILE = '/content/intent.json'\n",
        "\n",
        "try:\n",
        "    # Check if the file exists in the current directory (where Colab runs)\n",
        "    if not os.path.exists(INTENTS_FILE):\n",
        "        print(f\"Error: The file '{INTENTS_FILE}' was not found.\")\n",
        "        print(\"Please upload your 'Intent.json' file to the Colab session's file panel.\")\n",
        "        raise FileNotFoundError(f\"{INTENTS_FILE} not found.\")\n",
        "\n",
        "    with open(INTENTS_FILE, encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    print(f\"Successfully loaded data from {INTENTS_FILE}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the data: {e}\")\n",
        "    # Exit gracefully if data loading fails\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Initialize stemmer and download NLTK punkt\n",
        "stemmer = LancasterStemmer()\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK punkt...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Try to find punkt_tab and download if not found\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK punkt_tab...\")\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# --- 2. Preprocessing and Feature Extraction ---\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!', '.', ',', \"'s\", \"'re\", \"'m\"]\n",
        "\n",
        "# Loop through each sentence in the patterns\n",
        "for intent_data in data['intents']:\n",
        "    tag = intent_data['intent']  # Use 'intent' key for the tag\n",
        "    classes.append(tag)\n",
        "\n",
        "    for pattern in intent_data['text']: # Use 'text' key for patterns\n",
        "        # Tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # Add to word list\n",
        "        words.extend(w)\n",
        "        # Add to documents in our corpus\n",
        "        documents.append((w, tag))\n",
        "\n",
        "# Stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w.lower() not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# Remove duplicate classes and sort\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(f\"Total unique stemmed words (Vocabulary size): {len(words)}\")\n",
        "print(f\"Total intents/classes: {len(classes)}\")\n",
        "print(f\"Number of documents/patterns: {len(documents)}\")\n",
        "\n",
        "# Create training data\n",
        "training = []\n",
        "output = []\n",
        "# Create an empty array for the output (one-hot encoding)\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# Training set, bag of words for each pattern\n",
        "for doc in documents:\n",
        "    # Initialize our bag of words\n",
        "    bag = []\n",
        "    # List of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # Stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "\n",
        "    # Create the bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # Output is a '0' for each class and '1' for the current intent's class\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "# Convert training data to numpy arrays\n",
        "train_x = np.array(training)\n",
        "train_y = np.array(output)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition and Training ---\n",
        "\n",
        "# Build the Neural Network Model\n",
        "# Using a 3-layer fully connected network (Deep Neural Network)\n",
        "model = Sequential([\n",
        "    # Input layer and first hidden layer\n",
        "    Dense(128, input_shape=(len(train_x[0]),), activation='relu'),\n",
        "    Dropout(0.5), # Regularization to prevent overfitting\n",
        "    # Second hidden layer\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    # Output layer (number of neurons equals number of classes/intents)\n",
        "    Dense(len(train_y[0]), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nStarting model training (200 epochs)...\")\n",
        "# Train the model (set verbose=1 to see training progress)\n",
        "history = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=0)\n",
        "print(\"Model training complete!\")\n",
        "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "\n",
        "# --- 4. Prediction Functions ---\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "    \"\"\"Tokenizes and stems the sentence.\"\"\"\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bag_of_words(sentence, words, show_details=False):\n",
        "    \"\"\"Creates a bag of words (vector) for the input sentence.\"\"\"\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # Bag of words matrix\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(f\"found in bag: {w}\")\n",
        "    return np.array(bag)\n",
        "\n",
        "def classify_intent(sentence):\n",
        "    \"\"\"Predicts the intent tag for the sentence.\"\"\"\n",
        "    # Generate predictions from the model\n",
        "    input_data = bag_of_words(sentence, words)\n",
        "    # The model expects a batch, so we reshape the single input array\n",
        "    # We use a try-except block in case the input data shape is empty\n",
        "    try:\n",
        "        results = model.predict(np.array([input_data]), verbose=0)[0]\n",
        "    except Exception:\n",
        "        # If prediction fails (e.g., empty input), return empty list\n",
        "        return []\n",
        "\n",
        "    # Filter out predictions below a threshold\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
        "    # Sort by probability in descending order\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1])) # tag, probability\n",
        "    # Return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "    \"\"\"Gets a random response based on the highest-confidence intent.\"\"\"\n",
        "    if not intents_list:\n",
        "        return \"I'm sorry, I don't understand that. Can you rephrase?\"\n",
        "\n",
        "    tag = intents_list[0][0]\n",
        "    list_of_intents = intents_json['intents']\n",
        "\n",
        "    # Locate the intent object by matching the 'intent' key\n",
        "    for i in list_of_intents:\n",
        "        if i['intent'] == tag:\n",
        "            # We use the standard 'responses' field for simplicity.\n",
        "            # Handling extensions (like %%HUMAN%% replacements) is a next step!\n",
        "            result = random.choice(i['responses'])\n",
        "            return result\n",
        "    return \"Something went wrong fetching the response.\"\n",
        "\n",
        "\n",
        "# --- 5. Chat Loop Interface ---\n",
        "\n",
        "print(\"\\n--- ChatBot Ready ---\")\n",
        "print(f\"Model trained on {len(classes)} intents. Start chatting!\")\n",
        "print(\"Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # 1. Classify the user input (get intent tag)\n",
        "        intent_results = classify_intent(user_input)\n",
        "\n",
        "        # 2. Get a random response based on the intent\n",
        "        bot_response = get_response(intent_results, data)\n",
        "\n",
        "        print(f\"Bot: {bot_response}\")\n",
        "\n",
        "    except EOFError:\n",
        "        # Handles case where input is closed (common in scripted environments)\n",
        "        print(\"\\nChatBot session ended (EOF).\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        time.sleep(1)\n",
        "        break\n",
        "\n",
        "print(\"ChatBot session ended.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from /content/intent.json.\n",
            "Downloading NLTK punkt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK punkt_tab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique stemmed words (Vocabulary size): 156\n",
            "Total intents/classes: 42\n",
            "Number of documents/patterns: 138\n",
            "\n",
            "Starting model training (200 epochs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete!\n",
            "Final training accuracy: 0.9855\n",
            "\n",
            "--- ChatBot Ready ---\n",
            "Model trained on 42 intents. Start chatting!\n",
            "Type 'quit' to exit.\n",
            "You: bangladesh team t20 captain name?\n",
            "Bot: Captains in 2025: Shanto (Tests), Miraz (ODIs), Litton (T20Is).\n",
            "You: about shakib\n",
            "Bot: Career bests: Test 217, ODI 134*, T20I 84; best bowling 7/36 (Test), 5/29 (ODI), 5/20 (T20I).\n",
            "You: about mushfiq\n",
            "Bot: Key feats: first Bangladeshi with three Test double hundreds; ODI runs 7,795.\n",
            "You: tamim iqbal highest score in test\n",
            "Bot: Career highs: Test 206, ODI 158, T20I 103*.\n",
            "You: Mahmudullah date of birth\n",
            "Bot: Ireland tour (Bangladesh) 2025: Tests – Nov 11–15 (Sylhet), Nov 19–23 (Mirpur). T20Is – Nov 27 & 29 (Chattogram), Dec 2 (Mirpur).\n",
            "You: last test of mahmudullah\n",
            "Bot: Test squad vs Ireland (Nov 2025): Shanto (C), Shadman, Mahmudul Hasan Joy, Mominul, Mushfiqur, Litton, Jaker Ali, Miraz, Taijul, Syed Khaled, Hasan Mahmud, Nahid Rana, Ebadot, Hasan Murad.\n",
            "You: last series of bangladesh\n",
            "Bot: T20Is 2025: 3–0 win vs Afghanistan (UAE), 2–0 win vs Netherlands; mixed results vs Sri Lanka, Pakistan, West Indies; win vs Hong Kong and UAE.\n",
            "You: tell me about last one series of bd \n",
            "Bot: Ireland tour (Bangladesh) 2025: Tests – Nov 11–15 (Sylhet), Nov 19–23 (Mirpur). T20Is – Nov 27 & 29 (Chattogram), Dec 2 (Mirpur).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-294313835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rkpTa80YZ8yy",
        "outputId": "6339e1c6-3bc9-44e3-8e5b-c29b90cc0249"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}